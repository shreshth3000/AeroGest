{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "421615a7-953c-43a2-97bb-923e48213a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import speech_recognition as sr\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "d002d0dc-44ff-4275-93e4-275c66b01a29",
   "metadata": {},
   "outputs": [
    
   ],
   "source": [
    "print(\"Current Working Directory:\", os.getcwd())\n",
    "\n",
    "load_dotenv()\n",
    "IMAGE_SIZE = (64, 64)  \n",
    "CSV_FILE = 'hand_gesture_data.csv'\n",
    "MODEL_FILE = 'hand_gesture_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "602b92e7-3dd0-4162-9f89-235a3d94b0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speechrec():\n",
    "    genai.configure(api_key=os.environ[\"API_KEY\"])\n",
    "    model=genai.GenerativeModel(\"gemini-1.5-flash\",\n",
    "                generation_config=genai.GenerationConfig(\n",
    "                    max_output_tokens=300,\n",
    "                    temperature=0.6\n",
    "                ))\n",
    "    drone_instructions=model.start_chat(history=[])\n",
    "    drone_instructions.send_message(\n",
    "        \"\"\"\n",
    "        ###CONTEXT###\n",
    "        You are to responsible for recognizing individual voice instructions to control \n",
    "        2 drones,\n",
    "        which may be referred to as left drone (drone 1) or right drone (drone 2),\n",
    "        which you will be fed as the output text of a speech recognizer.\n",
    "        Keep output short, in the prescribed format:\n",
    "        DESIRED_FORMAT: INSTRUCTIONS FOR DRONE 1: INSTRUCTIONS \\n \n",
    "        INSTRUCTIONS FOR DRONE 2: INSTRUCTIONS\n",
    "        Furthermore, you may be asked a SIMPLE YES OR NO QUESTION, which does\n",
    "        not necessarily have anything todo with drones.\n",
    "        IN THIS CASE:\n",
    "        DESIRED_FORMAT: YES (OR) NO\n",
    "        \"\"\"\n",
    "    )\n",
    "    while True:\n",
    "        r = sr.Recognizer()\n",
    "        r.energy_threshold+=2000\n",
    "        cont=int(input(\"Press 1 to continue or 0 to break:\\n \"))\n",
    "        if cont==1:\n",
    "            with sr.Microphone() as source:\n",
    "                r.adjust_for_ambient_noise(source)\n",
    "                print(\"Give instructions\")\n",
    "                audio = r.listen(source)\n",
    "            try:\n",
    "                rec_out = r.recognize_google(audio)\n",
    "                error={'drawn':'Drone','up':'UP','down':'DOWN','left':'LEFT',\n",
    "                       'right':'RIGHT','forward':'FORWARD','backward':'BACKWARD',\n",
    "                       'and':'AND'}\n",
    "                for i, j in error.items():\n",
    "                    rec_out=rec_out.replace(i,j)\n",
    "                print(drone_instructions.send_message(\n",
    "                    \"\"\"###INSTRUCTION###\n",
    "                    These are the instructions for the drones or the yes/no question\n",
    "                    you must answer: \n",
    "                    \"\"\"+\n",
    "                    rec_out\n",
    "                    ).text)\n",
    "            except sr.UnknownValueError:\n",
    "                print(\"Couldn't understand audio,sorry\")\n",
    "        elif cont==0:\n",
    "            break\n",
    "        else:\n",
    "            print(\"please try again\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "a41f3e2e-a55d-4367-91f3-8520e9874aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_images(gesture_name, num_images=200):\n",
    "    cap = cv.VideoCapture(0)\n",
    "    images = []\n",
    "    print(f\"Press 's' to start capturing {num_images} images for gesture: {gesture_name}. Press 'q' to stop early.\")\n",
    "    \n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        cv.imshow('Capture Hand Gesture', cv.flip(frame,1))\n",
    "        key = cv.waitKey(1) & 0xFF\n",
    "        if key == ord('s'):  \n",
    "            break\n",
    "        elif key == ord('q'):  \n",
    "            cap.release()\n",
    "            cv.destroyAllWindows()\n",
    "            return\n",
    "\n",
    "    \n",
    "    while len(images) < num_images:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    " \n",
    "        gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "        resized = cv.resize(gray, IMAGE_SIZE)\n",
    "        \n",
    " \n",
    "        cv.imshow('Capture Hand Gesture', cv.flip(frame,1))\n",
    "        \n",
    "        images.append(resized.flatten()) \n",
    "        \n",
    "        \n",
    "        time.sleep(0.25)\n",
    "        \n",
    "        \n",
    "        if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv.destroyAllWindows()\n",
    "    \n",
    "\n",
    "    df = pd.DataFrame(images)\n",
    "    df['label'] = gesture_name\n",
    "    df.to_csv(CSV_FILE, mode='a', header=not pd.io.common.file_exists(CSV_FILE), index=False)\n",
    "    print(f\"Saved {len(images)} images to {CSV_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "8b68c9b7-b6ef-4e1d-8201-e4efe2ae60bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    df = pd.read_csv(CSV_FILE)\n",
    "    X = df.iloc[:, :-1].values  \n",
    "    y = df.iloc[:, -1].values   \n",
    "    \n",
    "\n",
    "    X = X.reshape(-1, IMAGE_SIZE[0], IMAGE_SIZE[1], 1)\n",
    "    \n",
    "    \n",
    "    X = X / 255.0\n",
    "    \n",
    "    \n",
    "    y = pd.get_dummies(y).values  \n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57271fc5-e317-4b1e-a5eb-5026b1ada494",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    X, y = load_data()\n",
    "    \n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 1)),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.Conv2D(64,(5,5),activation='relu'),\n",
    "        layers.MaxPooling2D((2,2))\n",
    "        layers.Conv2D(32,(3,3),activation='relu'),\n",
    "        layers.Conv2D(32,(3,3),activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(16,activation='relu'),\n",
    "        layers.Dropout(0.1),\n",
    "        layers.Dense(y.shape[1], activation='softmax')  \n",
    "    ])\n",
    "    \n",
    "    \n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    model.fit(X, y, epochs=20, batch_size=16, validation_split=0.2)\n",
    "    \n",
    "    model.save(MODEL_FILE)\n",
    "    print(f\"Model saved to {MODEL_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "2f241fd4-133b-478f-9bfb-a25390486f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_gesture():\n",
    "    \n",
    "    model = tf.keras.models.load_model(MODEL_FILE)\n",
    "    df = pd.read_csv(CSV_FILE)\n",
    "    label_names = df['label'].unique()\n",
    "    cap = cv.VideoCapture(0)\n",
    "    print(\"Recognizing gestures. Press 'q' to quit.\")\n",
    "    print(label_names)\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "    \n",
    "        gray = cv.cvtColor(cv.flip(frame,1), cv.COLOR_BGR2GRAY)\n",
    "        resized = cv.resize(gray, IMAGE_SIZE)\n",
    "        normalized = resized / 255.0\n",
    "        input_data = normalized.reshape(1, IMAGE_SIZE[0], IMAGE_SIZE[1], 1)\n",
    "        \n",
    "    \n",
    "        predictions = model.predict(input_data)\n",
    "        print(predictions)\n",
    "        max=0.\n",
    "        if np.argmax(predictions)>=max:\n",
    "            predicted_label = label_names[np.argmax(predictions)]\n",
    "        \n",
    "    \n",
    "        text= cv.putText(cv.flip(frame,1), f\"Gesture: {predicted_label}\", (10, 30), cv.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "        cv.imshow('Recognize Hand Gesture', text)\n",
    "        \n",
    "    \n",
    "        if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594a15b7-3f27-4972-ac7f-e9b17ef1a00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    print(\"CHOOSE: 1 for capturing images and training the model, 2 for recognizing gestures, 3 to recognize speech, 4 to exit....\")\n",
    "    choice=int(input())\n",
    "    if choice==1:\n",
    "        \n",
    "        gesture_name = input(\"Enter the name of the gesture: \")\n",
    "        capture_images(gesture_name)\n",
    "        train_model()\n",
    "    if choice==2:\n",
    "       \n",
    "        recognize_gesture()\n",
    "    if choice==3:\n",
    "        speechrec()\n",
    "    if choice==4:\n",
    "        break\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
